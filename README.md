# PROJEKTIOPINNOT 2: Koneoppiminen (2024) - Mojovat

##### Projektin tekijÃ¤t ovat:

* Andreas Konga
* Joni Kauppinen
* Linnea Kauppinen
* Heidi Pappila
* Ville MÃ¶rsÃ¤ri

# Projektin kuvaus

TÃ¤ssÃ¤ projektissa pyritÃ¤Ã¤n tarjoamaan Kainuun Hyvinvointialueelle data-analyysipalveluja, jotka tukevat pÃ¤Ã¤tÃ¶ksentekoa ja parantavat palveluiden laatua. Analyysit keskittyvÃ¤t erityisesti henkilÃ¶stÃ¶n tyÃ¶tyytyvÃ¤isyyteen (NES) ja asiakastyytyvÃ¤isyyteen (HOPP), keskittyen tÃ¤rkeimpiin osa-alueisiin, kuten johtamiseen, sitoutuneisuuteen ja asiakaspalvelun laatuun. Tavoitteena on tuottaa hyÃ¶dyllistÃ¤ tietoa, joka auttaa Kainuun Hyvinvointialuetta parantamaan toimintansa tehokkuutta, kehittÃ¤mÃ¤Ã¤n henkilÃ¶stÃ¶n hyvinvointia ja asiakastyytyvÃ¤isyyttÃ¤ sekÃ¤ optimoimaan resurssien kÃ¤yttÃ¶Ã¤ alueen sosiaali- ja terveyspalveluissa sekÃ¤ pelastustoimessa.

Data-analyysit tarjoavat selkeitÃ¤ ja kÃ¤ytÃ¤nnÃ¶llisiÃ¤ nÃ¤kÃ¶kulmia, jotka tukevat strategisten pÃ¤Ã¤tÃ¶sten tekemistÃ¤ ja palveluiden kehittÃ¤mistÃ¤ Kainuun alueella, ja myÃ¶s ennustuksia tulevaan. NÃ¤iden ennusteiden avulla voidaan esimerkiksi arvioida henkilÃ¶stÃ¶- ja asiakastyytyvÃ¤isyyden kehityssuuntia, tunnistaa mahdollisia riskejÃ¤ ja varautua niihin ennakolta sekÃ¤ suunnitella resurssien allokointia entistÃ¤ tehokkaammin.

# Projektin rakenne
**SisÃ¤ltÃ¤Ã¤ projekti kansion rakenteen**

```
PROJECT_SIGMA/
â”‚
â”œâ”€â”€ app/                           # Container 3: Streamlit app
â”‚   â”œâ”€â”€ .streamlit/
â”‚   â”‚   â””â”€â”€ config.toml            # For streamlit configuration
â”‚   â”œâ”€â”€ src/                       # Source code for Streamlit
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â””â”€â”€ .webp              # Main page image
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ 1ğŸ©»NES Analyysi     # Contains basic analysis for NES data
â”‚   â”‚   â”‚   â”œâ”€â”€ 2ğŸ¥HOPP Analyysi    # Contains Logistic Regression analysis for HOPP data
â”‚   â”‚   â”‚   â”œâ”€â”€ 3ğŸ”¢Statistiikka.py  # Contains basic statics for Gold based HOPP data
â”‚   â”‚   â”‚   â”œâ”€â”€ 4ğŸ“‰Lineaarisuus     # Contains Linear Regression analysis for HOPP data
â”‚   â”‚   â”‚   â”œâ”€â”€ 5ğŸ“ŠLogistiikka.py   # Contains interactive visualisation for HOPP data
â”‚   â”‚   â”‚   â”œâ”€â”€ 6ğŸ› ï¸HOPP Tool        # Contains prediction tool based on Machine Learning models
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ”Data.py          # Contains 3 different tools for data
â”‚   â”‚   â”œâ”€â”€ ğŸ Etusivu.py           # Streamlit main page
â”‚   â”‚   â””â”€â”€ utils.py               # Utility functions for streamlit
â”‚   â”œâ”€â”€ Dockerfile                 # Dockerfile for Streamlit container
â”‚   â””â”€â”€ README.md                  # Instructions for Streamlit
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ HOPP/                      # HOPP Excel data
â”‚   â”œâ”€â”€ ingestion/                 # Contains ingested data
â”‚   â”‚   â””â”€â”€ .gitkeep               # Saves files 
â”‚   â”œâ”€â”€ NES/                       # NES Excel data
â”‚   â””â”€â”€ .gitkeep                   # Saves files 
â”‚
â”œâ”€â”€ database/                      # Container 2: CosmosDB Query service
â”‚   â”œâ”€â”€ Dockerfile                 # Dockerfile for database container
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ db_templates/              # SQL templates used for querying
â”‚   |   â””â”€â”€ bronze.sql             # Bronze template
â”‚   â”œâ”€â”€ src/                       # Source code for database query service
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connection_tool.py     # Connection module to database
â”‚   â”‚   â”œâ”€â”€ database_api.py        # Fast API for query service
â”‚   â”‚   â””â”€â”€ utils.py               # Utility functions for database api
â”‚ 
â”œâ”€â”€ diary/                         # Mkdocs folder
â”‚   â”œâ”€â”€ docs/                       
â”‚   â”‚   â”œâ”€â”€ docs/       
â”‚   â”‚   â”‚   â”œâ”€â”€ images/            # Folder contains all images used in Mkdocs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ .png
â”‚   â”‚   â”‚   â”œâ”€â”€ weeks/             # Folder contains all the Sprint and other documentation files
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ sprints        
â”‚   â”‚   â”‚   â”œâ”€â”€ .pages             # Required for pages
â”‚   â”‚   â”‚   â”œâ”€â”€ index.md           # First page
â”‚   â”‚   â”‚   â””â”€â”€ loppuraportti.md   # Final report / Last page
â”‚   â”‚   â””â”€â”€ mkdocs.yml
â”‚   â”œâ”€â”€ docker-compose-yml         # Mkdocs container
â”‚   â”œâ”€â”€ docs.Dockerfile            # Mkdocs dockerfile
â”‚   â””â”€â”€ HOW-TO-DOCS.md             # Markdown file for Mkdocs instructions
â”‚
â”œâ”€â”€ ingestion/                     # Container 1: Data ingestion
â”‚   â”œâ”€â”€ src/                       # Source code for ingestion
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion_api.py       # FastAPI ingestion  
â”‚   â”‚   â””â”€â”€ utils.py               # Utility functions for Ingestion tool
â”‚   â”œâ”€â”€ Dockerfile                 # Dockerfile for ingestion container
â”‚   â””â”€â”€ README.md                  # Readme file for data ingestion
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ .gitkeep                   # Saves files
â”‚   â”œâ”€â”€ db_connect.ipynb           # Database connection locally notebook
â”‚   â”œâ”€â”€ first_db_connection.ipynb  # Create new container to database
â”‚   â”œâ”€â”€ gold_EDA_pg.ipynb          # Gold table
â”‚   â”œâ”€â”€ hopp_example.csv           # HOPP data example file
â”‚   â”œâ”€â”€ HOPP_HOPP.ipynb            # HOPP data analysis notebook
â”‚   â”œâ”€â”€ lisÃ¤Ã¤_kÃ¤ppyrÃ¤Ã¤.ipynb       # HOPP data predictions notebook
â”‚   â”œâ”€â”€ nes_example.csv            # NES data example file
â”‚   â”œâ”€â”€ NES_NES.ipynb              # NES data analysis notebook
â”‚   â”œâ”€â”€ SatunnainenMettÃ¤.ipynb     # HOPP data with Random Forest model
â”‚   â””â”€â”€ silver_HOPPLOPP.ipynb      # Silver table API
â”‚
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html                 # To deploy documentation pages
â”‚
â”œâ”€â”€ shared/                        # Shared resources and configurations
â”‚   â”œâ”€â”€ requirements/              # Requirements files for each container
â”‚   â”‚   â”œâ”€â”€ ingestion.txt
â”‚   â”‚   â”œâ”€â”€ database.txt
â”‚   â”‚   |â”€â”€ app.txt
â”‚   â”‚   â””â”€â”€ local.txt
â”‚   â”œâ”€â”€ requirements.txt           # Requirements
â”‚   â””â”€â”€ .env                       # Environment variables
â”‚
â”œâ”€â”€ .gitignore                     # Git ignore
â”‚
â”œâ”€â”€ .gitlab-ci.yml                 # GitLab pipeline
â”‚
â”œâ”€â”€ docker-compose.yml             # Compose file to manage containers and networking
â”‚
â””â”€â”€ README.md                      # Project basics and instructions

```

# Sovelluksen tyÃ¶kalut

##### Listattu vaaditut tyÃ¶kalut sovelluksen kÃ¤yttÃ¶Ã¶n.

* Python
* Pip
* Git
* Docker
* Azure Cosmos SDK
* Streamlit

# Riippuvuksien ja tyÃ¶kalujen asennus ja kÃ¤yttÃ¶Ã¶notto

##### Ohjeet projektin riippuvuuksien asentamiseen sekÃ¤ Docker, Docker Compose ja Streamlit kÃ¤yttÃ¶Ã¶n.

### Riippuvuuksien asentaminen

1. **Asenna python virtuaali ympÃ¤ristÃ¶ komennolla:** 
```
pip install virtualenv
```
2. **Luo uusi virtuaaliympÃ¤ristÃ¶ komenolla:**
```
python -m venv .venv
```
3. **Aktivoi virtuaaliympÃ¤ristÃ¶ komennolla:**
```
# Windows:
.venv/Scripts/activate

# macOS/Linus:
source .venv/bin/activate
```
4. Vaihtoehto 1: **Asenna riippuvuudet kÃ¤yttÃ¤mÃ¤llÃ¤ komentoa:**
```
pip install -r requirements.txt
```
4. Vaihtoehto 2: **Asenna riippuvuudet kÃ¤yttÃ¤mÃ¤llÃ¤ docker composea (ohjeet alla)**

### Docker Compose

**SisÃ¤ltÃ¤Ã¤ levykuvat ingestionille, tietokannalle ja Streamlitille**

1. **Rakenna levykuva ja kontti:**
```
docker-compose build
```
2. **Docker -kontin kÃ¤ynnistys:**
```
docker-compose up
```
3. **Dockerin alasajo:**
```
ctrl + c / docker-compose down
```

### CosmosDB (tietokanta) kÃ¤yttÃ¶ohje
**Kun Docker Compose kÃ¤ynnistetty**

1. **Navigoi selaimessa osoitteeseen:**

* http://localhost:8081

### Ingestion kÃ¤yttÃ¶ohje
**Kun Docker Compose kÃ¤ynnistetty**

1. **Navigoi selaimessa osoitteeseen:**

* http://localhost:8080

### Streamlit kÃ¤yttÃ¶ohje
**Kun Docker Compose kÃ¤ynnistetty**

1. **Navigoi selaimessa osoitteeseen:**

* http://localhost:8501


## DokumentointityÃ¶kalu

##### Projekti on dokumentoitu Markdown -pohjaisen MKDocs -tyÃ¶kalun avulla.

### MKDocs kÃ¤yttÃ¶ohje

1. **Navigoi oikeaan kansioon:**
```
cd diary
```

2. **KÃ¤ynnistÃ¤ Docker komennolla:**
```
docker compose -f docker-compose-docs.yml up
```
3. **Navigoi selaimessa osoitteeseen:**

* http://localhost:8000

4. **Dockerin alasajo:**
```
docker compose -f docker-compose-docs.yml down
```
# Ingestion

Uutta dataa tuodaan jÃ¤rjestelmÃ¤Ã¤n ingestoimalla Excel-tiedostoja. Streamlit-sivustolla kÃ¤yttÃ¤jÃ¤t voivat ladata Excel-tiedostoja ja lÃ¤hettÃ¤Ã¤ ne tietokantaan. Latauksen jÃ¤lkeen data kirjoitetaan paikallisesti JSON-tekstitiedostoon, joka toimii lake-tason tiedostona. Lakesta poimittu JSON-tiedosto lÃ¤hetetÃ¤Ã¤n HTTP-pyynnÃ¶llÃ¤ Ingestion-konttiin, joka suorittaa seuraavat vaiheet:

* **Datan validointi:** Tarkistaa, ettÃ¤ tiedot ovat oikeassa formaatissa ja ettÃ¤ niissÃ¤ ei ole kriittisiÃ¤ puutteita.
* **ID:n luominen:** Jokaiselle tietueelle generoidaan uniikki tunniste.

![alt text](./images/image.png)

# Bronze Pipeline
Ingestion-kontin validoima datasetti lÃ¤hetetÃ¤Ã¤n POST-pyynnÃ¶llÃ¤ tietokanta-kontin bronze endpointtiin. Esimerkki endpointista:
`/upload/bronze/hopp.`

Data tallennetaan sellaisenaan CosmosDB:n tietokannan bronze-tasolle, joka toimii raakadatavarastona ilman muokkausta.

## Query
```
SELECT * FROM c WHERE c['/medallion'] = 'bronze_hopp'
```

# Silver Pipeline

Kutsumalla rajapintaa /process/silver/hopp, kÃ¤ynnistetÃ¤Ã¤n dataputki, jossa bronze-tason data prosessoidaan PipelineManagerilla. TÃ¤ssÃ¤ vaiheessa suoritetaan seuraavat toimenpiteet:

1. Datan standardointi: Sarakkeiden nimet, formaatit ja datatyypit yhdenmukaistetaan.
2. PartitionKey:n lisÃ¤Ã¤minen: Jokaiselle tietueelle lisÃ¤tÃ¤Ã¤n partitionKey, joka mahdollistaa tehokkaan hakemisen ja tallennuksen CosmosDB:ssÃ¤.

Prosessoitu data ladataan CosmosDB:n silver-tasolle, joka edustaa jalostettua ja valmiiksi kÃ¤siteltyÃ¤ dataa.

## PartitionKey 
PartitionKey on olennainen komponentti CosmosDB:n tietojen hajautuksessa ja suorituskyvyn optimoinnissa. Sen avulla tiedot jaetaan loogisiin osioihin (logical partitions), jotka voivat jakautua eri fyysisiin osioihin (physical partitions). TÃ¤mÃ¤ varmistaa seuraavat edut:

* **Tasainen kuormanjako:** PartitionKey jakaa tiedot tasaisesti, mikÃ¤ estÃ¤Ã¤ yhden osion ylikuormituksen.
* **Nopea haku ja kirjoitus:** Kyselyt kohdistuvat suoraan oikeaan osioon ilman tarpeetonta koko tietokannan skannausta.

## Query
```
SELECT * FROM c WHERE c['/medallion'] = 'silver_hopp'
```

# Gold Pipeline

Gold-tason prosessi kÃ¤ynnistetÃ¤Ã¤n kutsumalla rajapintaa /process/gold/hopp. TÃ¤mÃ¤ vaihe kÃ¤yttÃ¤Ã¤ PipelineManageria suorittamaan seuraavat toiminnot:

1. Agregaation luominen: Gold-prosessi yhdistÃ¤Ã¤ ja jalostaa useita datasettiÃ¤.
2. Datasetin yhdistÃ¤minen: Kooste- ja paikalliskyselydata yhdistetÃ¤Ã¤n standardisoimalla sarakkeet.
3. Pivotointi ja analyysi:
    * Identtiset kysymyssarakkeet yhdistetÃ¤Ã¤n.
    * Arvojen keskiarvot lasketaan.
    * Data jaetaan kvartaaleihin ja datajoukkoihin

Gold-taso sisÃ¤ltÃ¤Ã¤ korkean tason analytiikkadataa, joka on valmis raportointiin ja pÃ¤Ã¤tÃ¶ksenteon tukemiseen. TÃ¤mÃ¤ vaihe takaa, ettÃ¤ data on tÃ¤ysin aggregoitua, standardoitua ja optimoitua loppukÃ¤yttÃ¶Ã¤ varten.

# SQL-kyselyt
Rajapinnan `/get/{layer}/{source}` avulla voidaan hakea tietoa CosmosDB:stÃ¤ mÃ¤Ã¤ritetyn kerroksen (layer) ja lÃ¤hteen (source) perusteella. Esimerkki kÃ¤ytÃ¶stÃ¤:

TÃ¤mÃ¤ kutsu:

1. Varmistaa, ettÃ¤ kerros (layer) on joko bronze, silver tai gold.
2. Varmistaa, ettÃ¤ lÃ¤hde (source) on hopp tai nes.
3. Hakee oikean SQL-kyselyn Jinja2-templaten avulla.
4. Suorittaa kyselyn CosmosDB:ssÃ¤ ja palauttaa tulokset JSON-muodossa.

## Kyselyiden suorittaminen
Voit tehdÃ¤ SQL-kyselyjÃ¤ paikallisesti Pythonilla, esim. notebookissa tai Streamlitin kautta rajapintaan `localhost:8081/get/gold/hopp`. Huomioi, ettÃ¤ jos ajat StreamlitiÃ¤ kontissa, kaikki API kyselyt on tehtÃ¤vÃ¤ `project-network`-sillan kautta. TÃ¤llÃ¶in paikallisen osoitteen `localhost:8081` sijaan kyselyt ohjataan `database:8081/`-URLiin.

### Python kirjastot
Luo virtuaaliympÃ¤ristÃ¶
```bash
python3 -m venv .venv
```

Aktivoi virtuaaliympÃ¤ristÃ¶
```bash
source .venv/bin/activate  ## MAC Tms
source .venv/Scripts/activate ## Win 
```
tai
```powershell
.\.venv\Scripts\Activate.ps1
```
tai
```cmd
.\.venv\Scripts\activate.bat
```

Asenna python kirjastot
```bash
python -m pip install --upgrade pip
python -m pip install -r shared/requirements.txt
```

### .env asennus

.env -tiedostoon lisÃ¤tÃ¤Ã¤n tarvittavia muuttujia, esimerkiksi tietokanta yhteyksiÃ¤ varten tarvittavat URL:it, avaimet ja salasanat voidaan lisÃ¤tÃ¤
.env -tiedostoon. SiirrÃ¤ .env tiedosto ./shared/ -kansioon.

### Tietokanta API:n ajo

Aktivoi ensin terminaalissa python .venv, jonk asensit aiemmin. KÃ¤ynnistÃ¤ sen jÃ¤lkeen Tietokanta API:
```bash
python database/run_api.py
```

### Tietokanta docker ajo

Alla olevan komennon avulla voit buildaa tietokanta API dockerin
```bash
docker build -t database-api -f ./database/Dockerfile .
```

NÃ¤in ajat kyseisen docker imagen
```bash
docker run -d --name database-api -p 8082:8082 database-api
```

NÃ¤in poistat
```bash
docker stop database-api
docker rm database-api
```

### Tietokanta API:n kÃ¤yttÃ¶

Voit hekea esimerkiksi Bronze-tason datat. Kyselyt voit ajaa suoraan selaimesta, ainakin Chromen Pretty Print ominaisuus on ison datamÃ¤Ã¤rÃ¤n kanssa kÃ¤tevÃ¤.
```
http://localhost:8082/get/silver/nes
```
formaatti on
```
http://localhost:8082/get/medallion/source
```

Voit ajaa Silver pipeline datat
```
http://localhost:8082/process/silver/hopp
```
Silver pipeline ajo, lataa Bronze-tason datat, ajaa sille Transformaatiot ja tallentaa tietokantaan. 
Formaatti on vastaava, kuin yllÃ¤. 


## Exit / Sulku 
```
Ctrl + C
```

